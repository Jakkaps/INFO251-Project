{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "from settings import IMAGE_HEIGHT, IMAGE_WIDTH, OUT_DIR\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Create output directory\n",
    "if not os.path.exists(OUT_DIR):\n",
    "    os.mkdir(OUT_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from time import time\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from core.models.nts_net import NTSModel\n",
    "from core.loss import list_loss, ranking_loss\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import shutil\n",
    "\n",
    "def train(train, val, n_classes, epochs, batch_size, hr, scheduler_gamma=0.5):\n",
    "\n",
    "    # Identify device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Setup model output dir\n",
    "    folder_name = \"train_{}\".format(int(time()))\n",
    "    out_path = os.path.join(OUT_DIR, folder_name)\n",
    "    os.mkdir(out_path)\n",
    "\n",
    "    # Setup dataloader\n",
    "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    model = NTSModel(top_n=hr[\"proposal_num\"], n_classes=n_classes, image_height=IMAGE_HEIGHT, image_width=IMAGE_WIDTH).to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Setup optimizers\n",
    "    resnet_parameters = list(model.resnet.parameters())\n",
    "    navigator_parameters = list(model.navigator.parameters())\n",
    "    concat_parameters = list(model.concat_net.parameters())\n",
    "    partcls_parameters = list(model.partcls_net.parameters())\n",
    "\n",
    "    resnet_optim_params = {\"lr\": hr[\"resnet_lr\"], \"weight_decay\": hr[\"resnet_weight_decay\"], \"momentum\": hr[\"resnet_momentum\"]}\n",
    "    navigator_optim_params = {\"lr\": hr[\"navigator_lr\"], \"weight_decay\": hr[\"navigator_weight_decay\"], \"momentum\": hr[\"navigator_momentum\"]}\n",
    "    concat_optim_params = {\"lr\": hr[\"concat_lr\"], \"weight_decay\": hr[\"concat_weight_decay\"], \"momentum\": hr[\"concat_momentum\"]}\n",
    "    partcls_optim_params = {\"lr\": hr[\"partcls_lr\"], \"weight_decay\": hr[\"partcls_weight_decay\"], \"momentum\": hr[\"partcls_momentum\"]}\n",
    "\n",
    "    resnet_optimizer = torch.optim.SGD(resnet_parameters, **resnet_optim_params)\n",
    "    navigator_optimizer = torch.optim.SGD(navigator_parameters, **navigator_optim_params)\n",
    "    concat_optimizer = torch.optim.SGD(concat_parameters, **concat_optim_params)\n",
    "    partcls_optimizer = torch.optim.SGD(partcls_parameters, **partcls_optim_params)\n",
    "\n",
    "    # Setup learning rate scheduler\n",
    "    scheduler_interval = [int(epochs*0.25), int(epochs*0.5), int(epochs*0.75)]\n",
    "\n",
    "    schedulers = [MultiStepLR(resnet_optimizer, milestones=scheduler_interval, gamma=scheduler_gamma),\n",
    "                MultiStepLR(navigator_optimizer, milestones=scheduler_interval, gamma=scheduler_gamma),\n",
    "                MultiStepLR(concat_optimizer, milestones=scheduler_interval, gamma=scheduler_gamma),\n",
    "                MultiStepLR(partcls_optimizer, milestones=scheduler_interval, gamma=scheduler_gamma)]\n",
    "\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"train_accuracy\": [],\n",
    "        \"val_accuracy\": [],\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        for scheduler in schedulers:\n",
    "            scheduler.step()\n",
    "\n",
    "        epoch_loss = 0\n",
    "        epoch_accuracy = 0\n",
    "        epoch_val_loss = 0\n",
    "        epoch_val_accuracy = 0\n",
    "        with tqdm(total=len(train_loader)) as pbar:\n",
    "            for i, (inputs, labels) in enumerate(train_loader):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                batch_size = inputs.size(0)\n",
    "\n",
    "                resnet_optimizer.zero_grad()\n",
    "                navigator_optimizer.zero_grad()\n",
    "                concat_optimizer.zero_grad()\n",
    "                partcls_optimizer.zero_grad()\n",
    "\n",
    "                resnet_logits, concat_logits, part_logits, top_n_idxs, top_n_proba = model(inputs)\n",
    "                \n",
    "                # Losses\n",
    "                resnet_loss = criterion(resnet_logits, labels)\n",
    "                navigator_loss = list_loss(part_logits.view(batch_size * hr[\"proposal_num\"], -1),\n",
    "                                        labels.unsqueeze(1).repeat(1, hr[\"proposal_num\"]).view(-1)).view(batch_size, hr[\"proposal_num\"])\n",
    "                concat_loss = criterion(concat_logits, labels)\n",
    "                rank_loss = ranking_loss(top_n_proba, navigator_loss, proposal_num=hr[\"proposal_num\"])\n",
    "                partcls_loss = criterion(part_logits.view(batch_size * hr[\"proposal_num\"], -1),\n",
    "                                    labels.unsqueeze(1).repeat(1, hr[\"proposal_num\"]).view(-1))\n",
    "                \n",
    "                loss = resnet_loss + concat_loss + rank_loss + partcls_loss\n",
    "                loss.backward()\n",
    "\n",
    "                resnet_optimizer.step()\n",
    "                navigator_optimizer.step()\n",
    "                concat_optimizer.step()\n",
    "                partcls_optimizer.step()\n",
    "\n",
    "                accuracy = (concat_logits.argmax(dim=1) == labels).float().mean()\n",
    "                \n",
    "                epoch_loss += concat_loss.item()\n",
    "                epoch_accuracy += accuracy.item()\n",
    "\n",
    "                pbar.set_postfix_str(\"Train loss: {:.4f}, Train accuracy: {:.4f}\".format(epoch_loss / (i+1), epoch_accuracy / (i+1)))\n",
    "                pbar.update(1)\n",
    "\n",
    "        with tqdm(total=(len(val_loader))) as pbar:\n",
    "            with torch.no_grad():\n",
    "                for i, (inputs, labels) in enumerate(val_loader):\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    batch_size = inputs.size(0)\n",
    "\n",
    "                    _, concat_logits, _, _, _ = model(inputs)\n",
    "\n",
    "                    concat_loss = criterion(concat_logits, labels)\n",
    "                    \n",
    "\n",
    "                    accuracy = (concat_logits.argmax(dim=1) == labels).float().mean()\n",
    "\n",
    "                    epoch_val_loss += concat_loss.item()\n",
    "                    epoch_val_accuracy += accuracy.item()\n",
    "\n",
    "\n",
    "                    pbar.set_postfix_str(\"Val loss: {:.4f}, Val accuracy: {:.4f}\".format(epoch_val_loss / (i+1), epoch_val_accuracy / (i+1)))\n",
    "                    pbar.update(1)\n",
    "\n",
    "        epoch_loss = epoch_loss/len(train_loader)\n",
    "        epoch_val_loss = epoch_val_loss/len(val_loader)\n",
    "\n",
    "        epoch_accuracy = epoch_accuracy/len(train_loader)\n",
    "        epoch_val_accuracy = epoch_val_accuracy/len(val_loader)\n",
    "\n",
    "        history[\"train_loss\"].append(epoch_loss)\n",
    "        history[\"val_loss\"].append(epoch_val_loss)    \n",
    "\n",
    "        history[\"train_accuracy\"].append(epoch_accuracy)\n",
    "        history[\"val_accuracy\"].append(epoch_val_accuracy) \n",
    "\n",
    "        print(f\"Epoch {epoch+1} - Loss: {epoch_loss:.4f} - Accuracy: {epoch_accuracy:.4f} - Val Loss: {epoch_val_loss:.4f} - Val Accuracy: {epoch_val_accuracy:.4f}\")\n",
    "\n",
    "        torch.save({\n",
    "            \"train_accuracy\": history[\"train_accuracy\"][-1],\n",
    "            \"val_accuracy\": history[\"val_accuracy\"][-1],\n",
    "            \"proposal_num\": hr[\"proposal_num\"],\n",
    "            \"n_classes\": n_classes,\n",
    "            \"state_dict\": model.module.state_dict(),\n",
    "        }, os.path.join(out_path, f\"epoch_{epoch+1}.ckpt\"))\n",
    "\n",
    "\n",
    "    # Remove all but the best checkpoints\n",
    "    best_epoch_idx = np.argmax(history[\"val_accuracy\"])\n",
    "    os.rename(os.path.join(out_path, f\"epoch_{best_epoch_idx + 1}.ckpt\"), os.path.join(out_path, \"model.ckpt\"))\n",
    "\n",
    "    for file in glob.glob(f\"{out_path}/**\"):\n",
    "        if not file.endswith('model.ckpt'):    \n",
    "                os.remove(file)\n",
    "\n",
    "    # Update latest model weights\n",
    "    src = os.path.join(out_path, \"model.ckpt\")\n",
    "    dst = os.path.join(OUT_DIR, \"latest_model.ckpt\")\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "    # Report best\n",
    "    print(\"Best epoch:\", best_epoch_idx+1)\n",
    "    print(\"Best val accuracy:\", history[\"val_accuracy\"][best_epoch_idx])\n",
    "\n",
    "    return history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, Resize, ToTensor, RandomHorizontalFlip, RandomCrop\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torchvision.datasets import FGVCAircraft\n",
    "from PIL import Image\n",
    "\n",
    "transform = Compose([\n",
    "  Resize((IMAGE_HEIGHT, IMAGE_WIDTH), Image.BILINEAR),\n",
    "  ToTensor(),\n",
    "])\n",
    "\n",
    "augment_transform = Compose([\n",
    "    Resize((int(IMAGE_HEIGHT * 1.5), int(IMAGE_WIDTH * 1.5)), Image.BILINEAR),\n",
    "    RandomCrop((IMAGE_HEIGHT, IMAGE_WIDTH)),\n",
    "    RandomHorizontalFlip(),\n",
    "    ToTensor(),\n",
    "])\n",
    "\n",
    "val_transform = Compose([\n",
    "  Resize((IMAGE_HEIGHT, IMAGE_WIDTH), Image.BILINEAR),\n",
    "  ToTensor(),\n",
    "])\n",
    "\n",
    "# Load data\n",
    "train_data = FGVCAircraft(root=\"data\", split=\"train\", transform=transform, download=True)\n",
    "n_classes = len(train_data.classes)\n",
    "augmented_data = FGVCAircraft(root=\"data\", split=\"train\", transform=augment_transform, download=True)\n",
    "\n",
    "train_data = ConcatDataset([train_data, augmented_data])\n",
    "val_data = FGVCAircraft(root=\"data\", split=\"val\", transform=val_transform, download=True)\n",
    "\n",
    "print(\"Train data size:\", len(train_data))\n",
    "print(\"Val data size:\", len(val_data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Load hyperparameters\n",
    "with open(\"hyperparameters.json\", \"r\") as f:\n",
    "    hr = json.load(f)\n",
    "\n",
    "history = train(train_data, val_data, n_classes=n_classes, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, hr=hr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot train history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "xticks = np.arange(1, len(history[\"train_accuracy\"]) + 1)\n",
    "\n",
    "ax[0].plot(history[\"train_accuracy\"], label=\"train\")\n",
    "ax[0].plot(history[\"val_accuracy\"], label=\"val\")\n",
    "ax[0].set_xticks(np.arange(len(history[\"train_accuracy\"])), xticks)\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Accuracy\")\n",
    "ax[0].legend()\n",
    "ax[0].set_title(\"Train vs. Val accuracy\")\n",
    "\n",
    "ax[1].plot(history[\"train_loss\"], label=\"train\")\n",
    "ax[1].plot(history[\"val_loss\"], label=\"val\")\n",
    "ax[1].set_xticks(np.arange(len(history[\"train_accuracy\"])), xticks)\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].set_ylabel(\"Loss\")\n",
    "ax[1].legend()\n",
    "ax[1].set_title(\"Train vs. Val Loss\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "info",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
